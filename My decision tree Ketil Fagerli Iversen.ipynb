{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the banknote data as 'data'. A single datapoint corresponds to one row of the txt file.\n",
    "each feature are separated by commas, the last integer value for each row is the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"data_banknote_authentication.txt\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i get a feel for what the data i will be working with, looks like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1372, 5)\n",
      "[[  3.6216    8.6661   -2.8073   -0.44699   0.     ]\n",
      " [  4.5459    8.1674   -2.4586   -1.4621    0.     ]\n",
      " [  3.866    -2.6383    1.9242    0.10645   0.     ]\n",
      " ...\n",
      " [ -3.7503  -13.4586   17.5932   -2.7771    1.     ]\n",
      " [ -3.5637   -8.3827   12.393    -1.2823    1.     ]\n",
      " [ -2.5419   -0.65804   2.6842    1.1952    1.     ]]\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has 1372 seperate datapoints. Each datapoint has 4 features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test-Val split function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define a function which splits a dataset into two parts based on a given proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(data, split_proportion):\n",
    "    \n",
    "    #Given some data, split it into a dataset and a subdataset. Both datasets have their rows shuffled.\n",
    "\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    subdata_size    = round(split_proportion*len(data)) # Finds amount of data to put in the subdata-set\n",
    "    indices         = list(np.arange(0,len(data),1))    # Makes a list from 1 to the number of samples(rows) in data\n",
    "    subdata_indices = random.sample(population=indices,k=subdata_size) # Extracts a random sample of indices from this list\n",
    "    \n",
    "    subdata         = data[subdata_indices] # Subdata is the dataset with given indices\n",
    "    dataset         = np.delete(data,subdata_indices, axis=0) # Dataset is the data but with rows removed\n",
    "    np.random.shuffle(dataset) # Shuffle the dataset\n",
    "\n",
    "    return dataset, subdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the above 'data_split' function to split 'data' into 'training_data', 'test_data' and 'val_data'. The proportional sizes are 0.8, 0.16 and 0.04 respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1098, 5)\n",
      "(219, 5)\n",
      "(55, 5)\n"
     ]
    }
   ],
   "source": [
    "training_data, val_test_data = data_split(data, split_proportion = 0.2)\n",
    "val_data, test_data = data_split(val_test_data, split_proportion = 0.2)\n",
    "\n",
    "print(training_data.shape)\n",
    "print(val_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i've printed the shapes of the three datasets. As we can see, their sizes are as expected from their proportional size compared to the full dataset 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, validation and test features / labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project wants our data to be separated into a data matrix X and a label vector y. Major parts of my code was written around the fact that i treat the data matrix X and the label vector y as one big matrix. So in many places in my code, functions require the full data matrix (both X and y) however within the learn() function, i will require X and y seperately as inputs then concatenate them into one full data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = training_data[:,-1]\n",
    "val_labels = val_data[:,-1]\n",
    "test_labels = test_data[:,-1]\n",
    "\n",
    "training_features = training_data[:,:-1]\n",
    "val_features = val_data[:,:-1]\n",
    "test_features = test_data[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have separated the data into two parts, where X is subscripted by the name 'features' and the labels are subscripted with the name 'labels'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 - 1.2 Implement a decision tree learning algorithm from scratch + Add gini index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label uniformity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i create a function which takes some data as input, and checks if the labels in the data are uniform (if there is only one unique label). If thats the case, return True, otherwise False. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_label_uniformity(data): \n",
    "    \n",
    "    # Finds all the unique labels. We know the labels occupy the last column of the data\n",
    "    unique_labels = np.unique(data[:,-1]) \n",
    "    \n",
    "    # If there is only one unique label, the list will have one element, then return true, otherwise false\n",
    "    if len(unique_labels) == 1: \n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature uniformity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i create a function which takes some data as input, and checks wether or not all the feature values for each column are identical, if so return true, otherwise false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_feature_uniformity(data):\n",
    "    \n",
    "    rows, columns = data.shape\n",
    "    \n",
    "    # Initialize the number of uniform columns\n",
    "    num_uniform_feature_cols = 0\n",
    "    \n",
    "    # We skip over the last column as it is the label column\n",
    "    for column in range(columns-1):\n",
    "        \n",
    "        # What are the unique values for the given column\n",
    "        unique_features = np.unique(data[:,column])\n",
    "        \n",
    "        # If there is only one unique value, then the column is uniform\n",
    "        if len(unique_features) == 1:\n",
    "            num_uniform_feature_cols += 1\n",
    "    \n",
    "    # If the number of uniform columns are equal to the amount of feature columns, then the dataset\n",
    "    # has a uniform featureset\n",
    "    if num_uniform_feature_cols == (columns-1):\n",
    "        return True\n",
    "    \n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i create a function which takes some data as input and outputs a label prediction. This function will base its classification off the dominant label of the data. Obviously if the data has only one unique label, it will return a certain classification. However, if there are several different labels in the data, it will return the predicted label as the most dominant label in the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(data):\n",
    "    \n",
    "    # We know that the labels occupy the last column of the data\n",
    "    label_column = data[:,-1]\n",
    "    \n",
    "    # Finds all the unique labels and the amount for each\n",
    "    unique_labels, label_counts = np.unique(label_column, return_counts = True) \n",
    "    \n",
    "    # Finds the label with the highest count, then finds the index of the most majority label in unique_labels.\n",
    "    # Thus the majority label is the one with the highest count.\n",
    "    majority_label_count = np.amax(label_counts) \n",
    "    majority_label_index = np.where(label_counts == majority_label_count) \n",
    "    majority_label = unique_labels[majority_label_index] \n",
    "    \n",
    "    classification = majority_label\n",
    "    \n",
    "    # HOWEVER, i ran into a problem in the code where classify_data() would occationally return 2 values.\n",
    "    # This is because sometimes the data is such that theres an equal amount of each label.\n",
    "    # Thus when such a conflict occurs, select one label at random. Also i need that the classification variable is\n",
    "    # a 1x1 numpy array.\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    if len(classification) > 1:\n",
    "        classification = np.array([np.random.choice(classification)])\n",
    "    \n",
    "    return classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find all the ways to split the feature data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now create a function which finds all the possible ways to partition a given dataset. I will call it partitioning as opposed to splitting to avoid confusion, as the word split has already been used by the 'data_split' function.\n",
    "\n",
    "This function takes some data, and for each column, checks for all unique values and sorts them from smallest to largest (by using the np.unique() function), then for each such unique value, it computes the midpoint between each two consecutive values and enters them into an array called splits.\n",
    "\n",
    "Splits is then stored into a dictionary called 'possible_partitions' with the key of the numerical value of the current column. After it has iterated over all feature colums (not the label column obviously), it returns the dictionary.\n",
    "\n",
    "I use a dictionary here as opposed to an array because given a dataset with N rows, different columns might not necessarily have exactly N-1 ways to partition the data into two parts. Some colums might have many repeat values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_possible_partitions(data):\n",
    "    \n",
    "    # I want to use a dictionary here because the data can have different amounts of splits per column\n",
    "    possible_partitions = {} \n",
    "    \n",
    "    # I get the number of colums for the dataset by checking entry 2 in the .shape vector \n",
    "    columns = data.shape[1] \n",
    "    \n",
    "    # We dont want to split by the label column so we skip the last column\n",
    "    for column in range(columns - 1):\n",
    "        \n",
    "        # There might be repeat values in the column, so i will only look at unique values. This will also sort the column\n",
    "        unique_rows = np.unique(data[:,column]) \n",
    "        splits = []\n",
    "        \n",
    "        # We skip the last unique row because we will partition by the middle of two points (N-1 splits for N rows)\n",
    "        for row in range(len(unique_rows) - 1): \n",
    "\n",
    "            value = unique_rows[row]\n",
    "            next_value = unique_rows[row + 1]\n",
    "            inbetween_value = (value + next_value) / 2 # Select partition inbetween two consecutive unique values\n",
    "\n",
    "            # Append the median to the splits list\n",
    "            splits.append(inbetween_value)\n",
    "\n",
    "        # Appends the list of splits to the dictionary with the name(key) of the column number\n",
    "        possible_partitions[column] = splits \n",
    "\n",
    "\n",
    "    return possible_partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition data based on column and specified value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i create a function which takes some dataset, a 'branch_column' value and a 'branch_value' as input and outputs two different datasets, a lower branch and an upper branch. The upper branch includes all rows from the dataset, where the value in column number 'branch_column' lays above the value 'branch_value'. The opposite holds for the lower branch. This function in other words splits the data into two parts based on what column we want to split on and what value you want to split by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_branches(data, branch_column, branch_value):\n",
    "    \n",
    "    column = data[:,branch_column] # Data in the given column\n",
    "    rows, columns = data.shape # Number of rows and columns\n",
    "\n",
    "    lower_branch_data = np.zeros([1,columns]) # Need to initialize empty arrays for the code to work\n",
    "    upper_branch_data = np.zeros([1,columns]) # These are zero vectors with one row and 'columns' number of columns\n",
    "\n",
    "    # If the value in the given row and column is less or equal to the branch_value, then we put it into the lower branch,\n",
    "    # else if the value is greater, we put it into the upper branch.\n",
    "    # We cycle through all the rows to partition all the datapoints.\n",
    "    for row in range(rows):\n",
    "\n",
    "        if (data[row,branch_column] <= branch_value):\n",
    "            lower_branch_data = np.concatenate((lower_branch_data, [data[row]]), axis = 0)\n",
    "        elif (data[row,branch_column] > branch_value):\n",
    "            upper_branch_data = np.concatenate((upper_branch_data, [data[row]]), axis = 0)\n",
    "\n",
    "    upper_branch_data = np.delete(upper_branch_data, 0, axis = 0)# Delete the first row as we initialized the upper \n",
    "    lower_branch_data = np.delete(lower_branch_data, 0, axis = 0)# and lower branches with an empty row\n",
    "\n",
    "    return lower_branch_data, upper_branch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i define some functions to compute the entropy of given data. First i get a list of all the labels in the data. Then i find all the unique labels and the amount of each one. Then i loop over all unique labels and i use the definition for computing the entropy, to compute the entropy of the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(data):\n",
    "    \n",
    "    label_data = data[:,-1] # Collects a list of all the labels\n",
    "    unique_labels, label_counts = np.unique(label_data, return_counts = True) # Gives all the unique labels and their count\n",
    "\n",
    "    entropy = 0\n",
    "\n",
    "    for i in range(len(unique_labels)):\n",
    "        \n",
    "        # Probability to pick the label unique_label[i] from the list label_data\n",
    "        probability = label_counts[i] / len(label_data) \n",
    "        \n",
    "        # Use the definition of entropy to compute the entropy of data\n",
    "        entropy -= probability*np.log2(probability) \n",
    "        \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i define a function which can compute the entropy of a given partition. It takes a lower and upper branch as inputs then outputs the branching entropy, i.e the entropy of the given partitioning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_branch_entropy(upper_branch, lower_branch):\n",
    "    \n",
    "    # We want to compute the entropy of the given branching\n",
    "\n",
    "    p_upper_branch = len(upper_branch) / (len(upper_branch) + len(lower_branch)) # Probability of upper branch\n",
    "    p_lower_branch = len(lower_branch) / (len(upper_branch) + len(lower_branch)) # Probability of lower branch\n",
    "\n",
    "    H_upper_branch = compute_entropy(upper_branch) # Computes entropy of dataset\n",
    "    H_lower_branch = compute_entropy(lower_branch)\n",
    "    \n",
    "    # Use the definition of conditional entropy\n",
    "    branch_entropy = p_upper_branch*H_upper_branch + p_lower_branch*H_lower_branch \n",
    "\n",
    "    return branch_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the Gini index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i define some functions to compute the gini index of given data. First i get a list of all the labels in the data. Then i find all the unique labels and the amount of each one. Then i loop over all unique labels and i use the definition for the gini index, to compute the gini index of the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gini_index(data):\n",
    "    \n",
    "    label_data = data[:,-1] # Collects a list of all the labels\n",
    "    unique_labels, label_counts = np.unique(label_data, return_counts = True) # Gives all the unique labels and their count\n",
    "\n",
    "    gini_idx = 0\n",
    "\n",
    "    for i in range(len(unique_labels)):\n",
    "        \n",
    "        # Probability to pick the label unique_label[i] from the list label_data\n",
    "        probability = label_counts[i] / len(label_data) \n",
    "        \n",
    "        # Use the definition of the gini index\n",
    "        gini_idx += probability*(1-probability)\n",
    "    \n",
    "    return gini_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i define a function which can compute the gini index of a given partition. It takes a lower and upper branch as inputs then outputs the branching gini, i.e the gini index of the given partitioning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_branch_gini_index(lower_branch, upper_branch):\n",
    "    \n",
    "    # We want to compute the gini index of the given branching\n",
    "\n",
    "    p_upper_branch = len(upper_branch) / (len(upper_branch) + len(lower_branch)) # Probability of upper branch\n",
    "    p_lower_branch = len(lower_branch) / (len(upper_branch) + len(lower_branch)) # Probability of lower branch\n",
    "\n",
    "    G_upper_branch = compute_gini_index(upper_branch) # Computes gini index of dataset\n",
    "    G_lower_branch = compute_gini_index(lower_branch)\n",
    "    \n",
    "    # Compute the weighted sum of the gini indices\n",
    "    branch_gini_idx = p_upper_branch*G_upper_branch + p_lower_branch*G_lower_branch \n",
    "\n",
    "    return branch_gini_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine optimal branching for entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes some data and the 'potential_splits' dictionary as inputs, and outputs the most optimal column and the most optimal value to partition the data. The function also outputs the lowest entropy, however this will only be used for an example below.\n",
    "\n",
    "The most optimal partitioning is the one in which the labels in the upper and lower branches have the lowest overall entropy on average. Here i do not bother to compute the maximal amount of information gain, because it gives the same answer as finding the lowest overall entropy of partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_optimal_branching(data, potential_splits):\n",
    "    \n",
    "    rows, columns = data.shape\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for column in range(columns - 1): # Skip the last column as we will not partition on the label column\n",
    "        split_values = potential_splits[column] # Branch values to check\n",
    "        for split in range(len(split_values)):\n",
    "            \n",
    "            # Retrieves a lower branch and an upper branch by calling the data_branches() function with and \n",
    "            # partitioning on a given column and split_value\n",
    "            lower_branch, upper_branch = data_branches(data, branch_column = column, branch_value = split_values[split])\n",
    "            \n",
    "            # Computes the entropy of the partitioning \n",
    "            entropy_of_branching = compute_branch_entropy(upper_branch, lower_branch)\n",
    "\n",
    "            if i == 0: # Condition to ensure we have a startingpoint of comparison\n",
    "                lowest_entropy = entropy_of_branching\n",
    "\n",
    "                optimal_branch_column = column\n",
    "                optimal_branch_value = split_values[split]          \n",
    "\n",
    "            # If the branch partitioning offers a lower entropy, record the new lowest entropy\n",
    "            # Also record in what column and with what split_value this was achieved\n",
    "            elif entropy_of_branching < lowest_entropy: \n",
    "                lowest_entropy = entropy_of_branching   \n",
    "\n",
    "                optimal_branch_column = column\n",
    "                optimal_branch_value = split_values[split]\n",
    "\n",
    "            # Increment i by 1 so that we skip the startingpoint condition\n",
    "            i += 1\n",
    "\n",
    "    return optimal_branch_column, optimal_branch_value, lowest_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to test, i will print the lowest entropy, the optimal branch column and the optimal branch value. This is done by calling on the 'find_possible_partitions()' function on our training data, then i call on the 'determine_optimal_branching()' function. Recall, in this case training_data means a matrix consisting of both X features and y labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_splits = find_possible_partitions(training_data)\n",
    "optimal_branch_column, optimal_branch_value, lowest_entropy = determine_optimal_branching(training_data, potential_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lowest entropy is: 0.5633805580049813\n",
      "The optimal branch column: 0\n",
      "The optimal value to branch by is: 0.295535\n"
     ]
    }
   ],
   "source": [
    "print('The lowest entropy is: ' + str(lowest_entropy))\n",
    "print('The optimal branch column: ' + str(optimal_branch_column))\n",
    "print('The optimal value to branch by is: ' + str(optimal_branch_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the best way to partition the data (via lowest entropy) is by splitting it in two by column number 0 and by value 0.295535. Thus every row in the upper_branch will all have the values in column 0 be above 0.295535, and every row in the lower_branch will have the values in column 0 be below 0.295535. The entropy of this partitioning is 0.5634."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine optimal branching for the gini index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of modifying the previous 'determine_optimal_branching()' function to accomodate the gini index method, i will make a different function instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_optimal_branching_gini(data, potential_splits):\n",
    "    \n",
    "    rows, columns = data.shape\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for column in range(columns - 1): # Skip the last column as we will not partition on the label column\n",
    "        split_values = potential_splits[column] # Branch values to check\n",
    "        for split in range(len(split_values)):\n",
    "            \n",
    "            # Retrieves a lower branch and an upper branch by calling the data_branches() function with and \n",
    "            # partitioning on a given column and split_value\n",
    "            lower_branch, upper_branch = data_branches(data, branch_column = column, branch_value = split_values[split])\n",
    "            \n",
    "            # Computes the entropy of the partitioning \n",
    "            gini_of_branching = compute_branch_gini_index(upper_branch, lower_branch)\n",
    "\n",
    "            if i == 0: # Condition to ensure we have a startingpoint of comparison\n",
    "                lowest_gini = gini_of_branching\n",
    "\n",
    "                optimal_branch_column = column\n",
    "                optimal_branch_value = split_values[split]          \n",
    "\n",
    "            # If the branch partitioning offers a lower gini index, record the new lowest gini\n",
    "            # Also record in what column and with what split_value this was achieved\n",
    "            elif gini_of_branching < lowest_gini: \n",
    "                lowest_gini = gini_of_branching   \n",
    "\n",
    "                optimal_branch_column = column\n",
    "                optimal_branch_value = split_values[split]\n",
    "\n",
    "            # Increment i by 1 so that we skip the startingpoint condition\n",
    "            i += 1\n",
    "\n",
    "    return optimal_branch_column, optimal_branch_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create a leaf class. All the leaf class does, is to hold the property .predictions which gives a label prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaf:\n",
    "\n",
    "    def __init__(self, label):\n",
    "        \n",
    "        self.predictions = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision_node class holds the properties .question, .lower_branch_data, .upper_branch_data, lower_branch_node and upper_branch_node. The question variable refers to what question the decision node should \"ask\" the new data. The upper and lower data variables contains the all the rows for the lower and upper branches, wheras the upper and lower nodes contain the upper and lower subtrees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_node:\n",
    "\n",
    "    def __init__(self, question, lower_branch_data, upper_branch_data, lower_branch_node, upper_branch_node, majority_label):\n",
    "        \n",
    "        self.question = question\n",
    "        self.lower_branch_node = lower_branch_node\n",
    "        self.upper_branch_node = upper_branch_node\n",
    "        self.lower_branch_data = lower_branch_data\n",
    "        self.upper_branch_data = upper_branch_data\n",
    "        self.majority_label = majority_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now finally, we will recursively build the decision tree. The build_tree() function takes training features and training labels as input and returns either a Leaf object or a Decision_node object. If all the labels in data are the same (only one unique label), it will return a leaf with that label. If all the features in each feature column are identical, it will return a leaf with the majority label.\n",
    "\n",
    "Otherwise, it will call upon the 'find_possible_splits()' function to find all the possible ways to partition the data. Depending on the impurity measure being 'entropy' or 'gini', the function will call upon either the determine_optimal_branching() function or determine_optimal_branching_gini() function to find the optimal column to branch by and the optimal value to branch by. Then it defines the variable 'question' as a list which contains the optimal branch column, a comparison operator and the optimal branch value. This will be used later for printing the tree and also for classifying new data.\n",
    "\n",
    "Then the function gets the data which lands in the upper and lower branches with the 'data_branches()' function. Finally the function recursively calls upon itself by calling on the 'build_tree' function with the lower- and upper branches to create the subtrees 'upper_branch_node' and 'lower_branch_node'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(X_training, y_training, impurity_measure='entropy'):\n",
    "    \n",
    "    data = np.column_stack((X_training, y_training))\n",
    "    majority_label = classify_data(data)\n",
    "    \n",
    "    # If all labels in given data is the same, return leaf. Store the majority label in this leaf.\n",
    "    # In this case, the labels are all the same value, so the majority label is certain\n",
    "    if check_label_uniformity(data) == True: \n",
    "        return Leaf(majority_label)\n",
    "    \n",
    "    # If all features in given data is the same, return leaf. Store the majority label in this leaf.\n",
    "    # In this case, the labels can have different values, and thus the majority label can have some uncertainty\n",
    "    if check_feature_uniformity(data) == True:\n",
    "        return Leaf(majority_label)\n",
    "    \n",
    "    # Finds all possible splits for each column in the given data. Stores it in a dictionary.\n",
    "    potential_splits = find_possible_partitions(data)\n",
    "    \n",
    "    \n",
    "    # If impurity_measure is given as 'entropy', then we find the optimal branch column and branch value by\n",
    "    # minimizing the entropy.\n",
    "    # If impurity_measure is given as 'gini', then we find the optimal branch column and branch value by\n",
    "    # computing the gini index\n",
    "    if impurity_measure == 'entropy':\n",
    "        optimal_branch_column, optimal_branch_value, lowest_entropy = determine_optimal_branching(data, potential_splits)\n",
    "    \n",
    "    elif impurity_measure == 'gini':\n",
    "        optimal_branch_column, optimal_branch_value = determine_optimal_branching_gini(data, potential_splits)\n",
    "    \n",
    "    \n",
    "    # Question stores the column we split on and what value to split by. Will be used for printing the tree and prediction\n",
    "    question = [optimal_branch_column, '<=', optimal_branch_value] \n",
    "    \n",
    "    # Values which lie below the given 'optimal_branch_value', goes to the lower branch,\n",
    "    # the values which lie above go to the upper branch\n",
    "    lower_branch_data, upper_branch_data = data_branches(data, optimal_branch_column, optimal_branch_value)\n",
    "    \n",
    "    \n",
    "    # As our learn() function wants a data matrix X and a label vector y, we need to split the data matrices\n",
    "    lower_branch_features = lower_branch_data[:,:-1]\n",
    "    lower_branch_labels = lower_branch_data[:,-1]\n",
    "    upper_branch_features = upper_branch_data[:,:-1]\n",
    "    upper_branch_labels = upper_branch_data[:,-1]\n",
    "    \n",
    "    \n",
    "\n",
    "    # Recursively build the lower branch.\n",
    "    lower_branch_node = build_tree(lower_branch_features, lower_branch_labels, impurity_measure)\n",
    "\n",
    "    # Recursively build the upper branch.\n",
    "    upper_branch_node = build_tree(upper_branch_features, upper_branch_labels, impurity_measure)\n",
    "\n",
    "    # Return a decision node. Store the question, the upper branch and lower branch in this node.\n",
    "    return Decision_node(question, lower_branch_data, upper_branch_data,\n",
    "                         lower_branch_node, upper_branch_node, majority_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first learn() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now create the learn function we wanted. This code might seem a bit redundant as it does not do anything but calling the build_tree() function, however it will become apparent why i create it like this once we get to implementing a pruning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(X, y, impurity_measure='entropy'):\n",
    "\n",
    "    X_training = X\n",
    "    y_training = y\n",
    "\n",
    "    return build_tree(X_training, y_training, impurity_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want a way to print the decision tree in such a way i can check if the results are reasonable. In the following function we take a node (the decision tree, a sub tree or a single node) as input and then prints the information contain within the node. If the node is a Leaf node, we will print a label prediction. If the node is a decision node, we print the question contained within the decision node, then recursively call upon the 'tree_printer()' function for the lower and upper branches contained within the current node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_printer(node, indent=\"\"):\n",
    "    \n",
    "    # The indent variable is here so that each time the tree funciton is called, the indent\n",
    "    # grows by another space. That has the effect of adding an extra indentation \n",
    "    # to each layer of the tree, making it easier to read and interpret.\n",
    "    \n",
    "    # If the node we are looking at is a leaf node, print the prediction for the label.\n",
    "    if isinstance(node, Leaf):\n",
    "        print (indent + \"Predict\", node.predictions)\n",
    "        return\n",
    "\n",
    "    # Otherwise print the question at the decision node\n",
    "    print(indent + 'Is value at column: ' + str(node.question[0]) + str(node.question[1]) + str(node.question[2]) + '?')\n",
    "\n",
    "    # Recursively decend the tree down the lower branch \n",
    "    print(indent + '___ Value lies below:')\n",
    "    tree_printer(node.lower_branch_node, indent + \"  \")\n",
    "\n",
    "    # Recursively decend the tree down the upper branch\n",
    "    print(indent + '^^^ Value lies above:')\n",
    "    tree_printer(node.upper_branch_node, indent + \"  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of training and visualizing a tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i show how to train a decision tree and how to print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tree = learn(training_features, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is value at column: 0<=0.295535?\n",
      "___ Value lies below:\n",
      "  Is value at column: 1<=5.86535?\n",
      "  ___ Value lies below:\n",
      "    Is value at column: 2<=6.21865?\n",
      "    ___ Value lies below:\n",
      "      Is value at column: 1<=4.09405?\n",
      "      ___ Value lies below:\n",
      "        Predict [1.]\n",
      "      ^^^ Value lies above:\n",
      "        Is value at column: 1<=4.1572?\n",
      "        ___ Value lies below:\n",
      "          Predict [0.]\n",
      "        ^^^ Value lies above:\n",
      "          Predict [1.]\n",
      "    ^^^ Value lies above:\n",
      "      Is value at column: 1<=-3.16705?\n",
      "      ___ Value lies below:\n",
      "        Is value at column: 0<=-0.357315?\n",
      "        ___ Value lies below:\n",
      "          Predict [1.]\n",
      "        ^^^ Value lies above:\n",
      "          Predict [0.]\n",
      "      ^^^ Value lies above:\n",
      "        Predict [0.]\n",
      "  ^^^ Value lies above:\n",
      "    Is value at column: 0<=-3.4448999999999996?\n",
      "    ___ Value lies below:\n",
      "      Predict [1.]\n",
      "    ^^^ Value lies above:\n",
      "      Predict [0.]\n",
      "^^^ Value lies above:\n",
      "  Is value at column: 0<=1.7438500000000001?\n",
      "  ___ Value lies below:\n",
      "    Is value at column: 2<=-2.2721999999999998?\n",
      "    ___ Value lies below:\n",
      "      Is value at column: 1<=6.41995?\n",
      "      ___ Value lies below:\n",
      "        Predict [1.]\n",
      "      ^^^ Value lies above:\n",
      "        Predict [0.]\n",
      "    ^^^ Value lies above:\n",
      "      Is value at column: 3<=0.0922265?\n",
      "      ___ Value lies below:\n",
      "        Is value at column: 0<=0.42002?\n",
      "        ___ Value lies below:\n",
      "          Is value at column: 0<=0.400865?\n",
      "          ___ Value lies below:\n",
      "            Predict [0.]\n",
      "          ^^^ Value lies above:\n",
      "            Predict [1.]\n",
      "        ^^^ Value lies above:\n",
      "          Predict [0.]\n",
      "      ^^^ Value lies above:\n",
      "        Is value at column: 2<=1.91945?\n",
      "        ___ Value lies below:\n",
      "          Is value at column: 1<=3.5591749999999998?\n",
      "          ___ Value lies below:\n",
      "            Predict [1.]\n",
      "          ^^^ Value lies above:\n",
      "            Predict [0.]\n",
      "        ^^^ Value lies above:\n",
      "          Predict [0.]\n",
      "  ^^^ Value lies above:\n",
      "    Is value at column: 2<=-4.802?\n",
      "    ___ Value lies below:\n",
      "      Is value at column: 0<=3.91485?\n",
      "      ___ Value lies below:\n",
      "        Predict [1.]\n",
      "      ^^^ Value lies above:\n",
      "        Predict [0.]\n",
      "    ^^^ Value lies above:\n",
      "      Predict [0.]\n"
     ]
    }
   ],
   "source": [
    "tree_printer(my_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the tree seems to work, i define a function which will predict a label. This function will read a single row from a dataset and with the given tree, predict the label.\n",
    "\n",
    "If 'print_progress' is set to True, then it will print the relevant questions and answers during the process as the algorithm is predicting the label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_row, node, print_progress=False):\n",
    "    \n",
    "    # If the remaining node is a leaf node, return the prediction of this leaf node\n",
    "    if isinstance(node, Leaf):\n",
    "        if print_progress == True:\n",
    "            print('We have reached a leaf node, we predict the label is: ')\n",
    "        return node.predictions\n",
    "    \n",
    "    # Printing relevant question for current node\n",
    "    if print_progress == True:\n",
    "        print('Is the value in column ' + str(node.question[0]) + ' ' + str(node.question[1]) \n",
    "              + ' ' + str(node.question[2]) + '?')\n",
    "    \n",
    "    # If the split value in the question is greater or equal to the value in the data_row at the given column\n",
    "    # then go to the lower branch, otherwise go to the upper branch\n",
    "    if node.question[2] >= data_row[node.question[0]]:\n",
    "        if print_progress == True:\n",
    "            print('True')\n",
    "        return predict(data_row, node.lower_branch_node, print_progress)\n",
    "    else:\n",
    "        if print_progress == True:\n",
    "            print('False')\n",
    "        return predict(data_row, node.upper_branch_node, print_progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of how to use the predict function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test this classifier function on the first datapoint in the validation dataset. I will set the variable print_progress to True, because i want to see how the classifier got to its conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us predict the label of the following example; \n",
      "\n",
      "[Column 0, Column 1, Column 2, Column 3, Label]\n",
      "[-3.8053  2.4273  0.6809 -1.0871  1.    ]\n",
      "\n",
      "Is the value in column 0 <= 0.295535?\n",
      "True\n",
      "Is the value in column 1 <= 5.86535?\n",
      "True\n",
      "Is the value in column 2 <= 6.21865?\n",
      "True\n",
      "Is the value in column 1 <= 4.09405?\n",
      "True\n",
      "We have reached a leaf node, we predict the label is: \n",
      "[1.]\n",
      "This prediction was correct!\n"
     ]
    }
   ],
   "source": [
    "example = val_data[0]\n",
    "\n",
    "print('Let us predict the label of the following example; \\n')\n",
    "print(str('[Column 0, Column 1, Column 2, Column 3, Label]'))\n",
    "print(str(example) + '\\n')\n",
    "print(predict(example, my_tree, print_progress=True))\n",
    "\n",
    "if predict(example, my_tree) == example[-1]:\n",
    "    print('This prediction was correct!')\n",
    "else:\n",
    "    print('This prediction was false!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to classify all the datapoints in the validation dataset and then check if the decision tree \n",
    "predicted the correct label or not. This function will just loop over each row in a dataset, and then use the above predict() function to count how many incorrect and correct predictions there were then return an accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data_accuracy(data_features, data_labels, node):\n",
    "    \n",
    "    # Get the amount of rows in the feature data\n",
    "    rows = data_features.shape[0]\n",
    "    \n",
    "\n",
    "    # Initialize amount of correct and incorrect predictions\n",
    "    correct_predictions = 0\n",
    "    incorrect_predictions = 0\n",
    "    \n",
    "    incorrect_rows = []\n",
    "\n",
    "    for row in range(rows):\n",
    "\n",
    "        # The classify function returns an np.array with a single element so we just extract the value\n",
    "        prediction = predict(data_features[row], node)[0]\n",
    "\n",
    "        # If the prediction is equal to the actual label, then we get a correct prediction\n",
    "        if prediction == data_labels[row]:\n",
    "            correct_predictions += 1\n",
    "\n",
    "        # If not, we have an incorrect prediction\n",
    "        else:\n",
    "            incorrect_predictions += 1\n",
    "            incorrect_rows.append(row)\n",
    "\n",
    "    # The accuracy of our decision tree is then \n",
    "    accuracy = correct_predictions / (correct_predictions + incorrect_predictions)\n",
    "    \n",
    "    print('Correct predictions: ' + str(correct_predictions))\n",
    "    print('Incorrect predictions: ' + str(incorrect_predictions))\n",
    "    print('The incorrect predictions happened for datapoints: ' + str(incorrect_rows))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of decision tree with the entropy method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 215\n",
      "Incorrect predictions: 4\n",
      "The incorrect predictions happened for datapoints: [14, 46, 124, 214]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9817351598173516"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_data_accuracy(val_features,val_labels, my_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of decision tree with the gini index method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to test the decision tree algorithm using the gini index instead of the entropy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_2nd_tree = learn(training_features, training_labels, impurity_measure='gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 214\n",
      "Incorrect predictions: 5\n",
      "The incorrect predictions happened for datapoints: [46, 82, 124, 129, 214]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9771689497716894"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_data_accuracy(val_features,val_labels, my_2nd_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in this specific case, the Gini index method of determining optimal partitioning gives us a lower degree of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Add reduced-error pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduced error pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first define a function which can compute wether or not replacing a given subtree by a leaf with the majority label is benefitial for the pruning accuracy or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_subtree_benefit(prune_features, prune_labels, question, majority_label, lower_branch_node, upper_branch_node):\n",
    "    \n",
    "    # I want the pruning data to be one large matrix with the first columns being feature columns and the last being\n",
    "    # a label column\n",
    "    prune_data = np.column_stack((prune_features,prune_labels))\n",
    "    \n",
    "    # The prediction of the current node is the majority label here\n",
    "    prediction_leaf = majority_label\n",
    "    \n",
    "    # Initialize amount of correct and incorrect predictions for pruned subtree\n",
    "    correct_pruned = 0\n",
    "    incorrect_pruned = 0\n",
    "    \n",
    "    # Initialize amount of correct and incorrect predictions for unpruned subtree\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    \n",
    "    # For each row in the pruning data, check if the value of the prune data is above or below the branch_value\n",
    "    # contained in question[2], then make the prediction on either the upper or lower branch. If the prediction\n",
    "    # is correct, increment correct by 1, if not increment incorrect by 1\n",
    "    # if the prediction by pruning the subtree (prediction_leaf) is correct, increment correct_pruned by 1\n",
    "    # if not increment incorrect_pruned by 1. \n",
    "    for row in range(len(prune_labels)):\n",
    "\n",
    "        if question[2] >= prune_data[row,question[0]]:\n",
    "            prediction = lower_branch_node.predictions\n",
    "        else:\n",
    "            prediction = upper_branch_node.predictions   \n",
    "        \n",
    "        # Increment amount of correct / incorrect predictions for original subtree\n",
    "        if prediction == prune_labels[row]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "        \n",
    "        # Increment amount of correct / incorrect predictions for pruned subtree\n",
    "        if prediction_leaf == prune_labels[row]:\n",
    "            correct_pruned += 1\n",
    "        else:\n",
    "            incorrect_pruned += 1\n",
    "    \n",
    "    # Compute accuracy\n",
    "    prune_accuracy_pruned = correct_pruned / (correct_pruned + incorrect_pruned)\n",
    "    prune_accuracy = correct / (correct + incorrect)\n",
    "\n",
    "    # If the accuracy of the pruned tree is the same or better, return True, if not, False\n",
    "    if prune_accuracy_pruned >= prune_accuracy:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now want to modify the 'build_tree()' function. It should build a tree using training data, then use pruning data to replace each subtree in the tree if the accuracy of the pruning data is not adversly affected by replacing the subtree by a leaf node containing the majority label of the subtree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_prune(X_training,y_training, X_prune, y_prune, impurity_measure='entropy', prune=False):\n",
    "    \n",
    "    data = np.column_stack((X_training, y_training))\n",
    "    prune_data = np.column_stack((X_prune, y_prune))\n",
    "    \n",
    "    majority_label = classify_data(data)\n",
    "    \n",
    "    # If all labels in given data is the same, return leaf. Store the data in this leaf.\n",
    "    if check_label_uniformity(data) == True: \n",
    "        return Leaf(majority_label)\n",
    "    \n",
    "    # If all features in given data is the same, return leaf. Store the majority label in this leaf.\n",
    "    # In this case, the labels can have different values, and thus the majority label can have some uncertainty\n",
    "    if check_feature_uniformity(data) == True:\n",
    "        return Leaf(majority_label)\n",
    "    \n",
    "    # Finds all possible splits for each column in the given data. Stores it in a dictionary.\n",
    "    potential_splits = find_possible_partitions(data)\n",
    "    \n",
    "    \n",
    "    # If impurity_measure is given as 'entropy', then we find the optimal branch column and branch value by\n",
    "    # minimizing the entropy.\n",
    "    # If impurity_measure is given as 'gini', then we find the optimal branch column and branch value by\n",
    "    # computing the gini index\n",
    "    if impurity_measure == 'entropy':\n",
    "        optimal_branch_column, optimal_branch_value, lowest_entropy = determine_optimal_branching(data, potential_splits)\n",
    "    \n",
    "    elif impurity_measure == 'gini':\n",
    "        optimal_branch_column, optimal_branch_value = determine_optimal_branching_gini(data, potential_splits)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Question stores the column we split on and what value to split by. Will be used for printing the tree and prediction\n",
    "    question = [optimal_branch_column, '<=', optimal_branch_value] \n",
    "    \n",
    "    # Values which lie below the given 'optimal_branch_pruneue', goes to the lower branch,\n",
    "    # the values which lie above go to the upper branch\n",
    "    lower_branch_data, upper_branch_data = data_branches(data, optimal_branch_column, optimal_branch_value)\n",
    "    lower_branch_prune, upper_branch_prune = data_branches(prune_data, optimal_branch_column, optimal_branch_value)\n",
    "    \n",
    "    \n",
    "    # As our learn() function wants a data matrix X and a label vector y, we need to split the data matrices\n",
    "    lower_branch_features = lower_branch_data[:,:-1]\n",
    "    lower_branch_labels = lower_branch_data[:,-1]\n",
    "    upper_branch_features = upper_branch_data[:,:-1]\n",
    "    upper_branch_labels = upper_branch_data[:,-1]\n",
    "    \n",
    "    \n",
    "    lower_branch_prune_features = lower_branch_prune[:,:-1]\n",
    "    lower_branch_prune_labels = lower_branch_prune[:,-1]\n",
    "    upper_branch_prune_features = upper_branch_prune[:,:-1]\n",
    "    upper_branch_prune_labels = upper_branch_prune[:,-1]\n",
    "    \n",
    "\n",
    "    # Recursively build the lower branch.\n",
    "    lower_branch_node = build_tree_prune(lower_branch_features, lower_branch_labels,\n",
    "                                      lower_branch_prune_features, lower_branch_prune_labels, impurity_measure, prune)\n",
    "\n",
    "    # Recursively build the upper branch.\n",
    "    upper_branch_node = build_tree_prune(upper_branch_features, upper_branch_labels,\n",
    "                                      upper_branch_prune_features, upper_branch_prune_labels, impurity_measure, prune)\n",
    "    \n",
    "    # If both the lower and upper nodes are leaf nodes, and if we have any pruning data that has reached this point,\n",
    "    # then compute the pruning benefit. If pruning has a benefit on the accuracy of the pruning data, then\n",
    "    # make the current node into a leaf node with the majority label of the training data at this point.\n",
    "    if (prune == True) and isinstance(lower_branch_node, Leaf) and isinstance(upper_branch_node, Leaf) and (len(prune_data[:,-1]) > 0):\n",
    "        pruning_benefit = prune_subtree_benefit(prune_data[:,:-1], prune_data[:,-1], question,\n",
    "                                                majority_label, lower_branch_node, upper_branch_node)\n",
    "        if pruning_benefit == True:\n",
    "            return Leaf(majority_label)\n",
    "\n",
    "    \n",
    "    # Return a decision node. Store the question, the upper branch and lower branch in this node.\n",
    "    return Decision_node(question, lower_branch_data, upper_branch_data,\n",
    "                         lower_branch_node, upper_branch_node, majority_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The learn() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project asks us to have a learn function with the inputs X, y, impurity_measure and prune. Since i only want to split the training data into pruning data once, then i need to have the tree_builder_prune() function seperate from the learn() functon. This function simply stacks X and y back together into a big matrix, splits them into a training set and a pruning set, then we remove the features from the labels for both sets. Then we call upon the tree_builder_function()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(X, y, impurity_measure='entropy', prune=False):\n",
    "    \n",
    "    # The ratio of prune data by amount of total training data\n",
    "    split_proportion = 0.25\n",
    "    \n",
    "    # I want to combine the features and labels before splitting\n",
    "    data = np.column_stack((X, y))\n",
    "    \n",
    "    # Split the data into training data and purning data\n",
    "    training_data, prune_data = data_split(data, split_proportion)\n",
    "    \n",
    "    # Split the matrices back into feature matrices and label vectors\n",
    "    X_training = training_data[:,:-1]\n",
    "    y_training = training_data[:,-1]\n",
    "    \n",
    "    X_prune = prune_data[:,:-1]\n",
    "    y_prune = prune_data[:,-1]\n",
    "    \n",
    "    # Build the tree\n",
    "    return build_tree_prune(X_training, y_training, X_prune, y_prune, impurity_measure, prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Evaluate your algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to evaluate the decision tree using validation data and vary the hyperparameters and then select the most suitable model. We will train the decision tree using training data X_training and y_training. Recall that inside the learn() function, 25% of this training data is removed from the training data and instead used as pruning data. Then, once the tree is built, we will call the function predict_data_accuracy() with \\teval\\_features} \\textbf{val\\_labels} and the decision tree, to compute the trees accuracy on the validation data. I will vary the hyperparameters \\textbf{impurity\\_measure} and \\textbf{prune} and check the accuracy for all the four combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruned vs Unpruned & Entropy vs Gini Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tree = learn(training_features, training_labels, impurity_measure='entropy', prune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_tree_pruned = learn(training_features, training_labels, impurity_measure='entropy', prune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct and incorrect predictions for pruned tree: \n",
      "Correct predictions: 215\n",
      "Incorrect predictions: 4\n",
      "The incorrect predictions happened for datapoints: [46, 82, 124, 214]\n",
      "\n",
      " Correct and incorrect predictions for non-pruned tree: \n",
      "Correct predictions: 213\n",
      "Incorrect predictions: 6\n",
      "The incorrect predictions happened for datapoints: [46, 82, 95, 103, 124, 214]\n",
      "\n",
      " Accuracy pruned tree: \n",
      "0.9817351598173516\n",
      "\n",
      " Accuracy non-pruned tree: \n",
      "0.9726027397260274\n",
      "\n",
      " Accuracy difference: \n",
      "0.0091324200913242\n"
     ]
    }
   ],
   "source": [
    "print('Correct and incorrect predictions for pruned tree: ')\n",
    "accuracy_pruned = predict_data_accuracy(val_features, val_labels, my_tree_pruned)\n",
    "print('\\n Correct and incorrect predictions for non-pruned tree: ')\n",
    "accuracy_nonpruned = predict_data_accuracy(val_features, val_labels, my_tree)\n",
    "\n",
    "difference = accuracy_pruned - accuracy_nonpruned\n",
    "\n",
    "print('\\n Accuracy pruned tree: ')\n",
    "print(accuracy_pruned)\n",
    "print('\\n Accuracy non-pruned tree: ')\n",
    "print(accuracy_nonpruned)\n",
    "print('\\n Accuracy difference: ')\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tree_gini = learn(training_features, training_labels, impurity_measure='gini', prune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tree_pruned_gini = learn(training_features, training_labels, impurity_measure='gini', prune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct and incorrect predictions for pruned tree: \n",
      "Correct predictions: 215\n",
      "Incorrect predictions: 4\n",
      "The incorrect predictions happened for datapoints: [46, 82, 124, 214]\n",
      "\n",
      " Correct and incorrect predictions for non-pruned tree: \n",
      "Correct predictions: 214\n",
      "Incorrect predictions: 5\n",
      "The incorrect predictions happened for datapoints: [46, 82, 124, 129, 214]\n",
      "\n",
      " Accuracy pruned tree: \n",
      "0.9817351598173516\n",
      "\n",
      " Accuracy non-pruned tree: \n",
      "0.9771689497716894\n",
      "\n",
      " Accuracy difference: \n",
      "0.004566210045662156\n"
     ]
    }
   ],
   "source": [
    "print('Correct and incorrect predictions for pruned tree: ')\n",
    "accuracy_pruned = predict_data_accuracy(val_features, val_labels, my_tree_pruned_gini)\n",
    "print('\\n Correct and incorrect predictions for non-pruned tree: ')\n",
    "accuracy_nonpruned = predict_data_accuracy(val_features, val_labels, my_tree_gini)\n",
    "\n",
    "difference = accuracy_pruned - accuracy_nonpruned\n",
    "\n",
    "print('\\n Accuracy pruned tree: ')\n",
    "print(accuracy_pruned)\n",
    "print('\\n Accuracy non-pruned tree: ')\n",
    "print(accuracy_nonpruned)\n",
    "print('\\n Accuracy difference: ')\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is value at column: 0<=0.746345?\n",
      "___ Value lies below:\n",
      "  Is value at column: 1<=5.3646?\n",
      "  ___ Value lies below:\n",
      "    Is value at column: 0<=-0.36205?\n",
      "    ___ Value lies below:\n",
      "      Is value at column: 2<=6.21865?\n",
      "      ___ Value lies below:\n",
      "        Predict [1.]\n",
      "      ^^^ Value lies above:\n",
      "        Is value at column: 1<=-2.2431270000000003?\n",
      "        ___ Value lies below:\n",
      "          Predict [1.]\n",
      "        ^^^ Value lies above:\n",
      "          Predict [0.]\n",
      "    ^^^ Value lies above:\n",
      "      Is value at column: 2<=2.1670999999999996?\n",
      "      ___ Value lies below:\n",
      "        Is value at column: 2<=0.47011500000000006?\n",
      "        ___ Value lies below:\n",
      "          Predict [1.]\n",
      "        ^^^ Value lies above:\n",
      "          Is value at column: 1<=-0.4242105?\n",
      "          ___ Value lies below:\n",
      "            Predict [1.]\n",
      "          ^^^ Value lies above:\n",
      "            Predict [0.]\n",
      "      ^^^ Value lies above:\n",
      "        Predict [0.]\n",
      "  ^^^ Value lies above:\n",
      "    Is value at column: 0<=-3.35385?\n",
      "    ___ Value lies below:\n",
      "      Predict [1.]\n",
      "    ^^^ Value lies above:\n",
      "      Predict [0.]\n",
      "^^^ Value lies above:\n",
      "  Is value at column: 2<=-4.69065?\n",
      "  ___ Value lies below:\n",
      "    Is value at column: 0<=3.91485?\n",
      "    ___ Value lies below:\n",
      "      Predict [1.]\n",
      "    ^^^ Value lies above:\n",
      "      Predict [0.]\n",
      "  ^^^ Value lies above:\n",
      "    Is value at column: 0<=1.7438500000000001?\n",
      "    ___ Value lies below:\n",
      "      Is value at column: 2<=-2.26365?\n",
      "      ___ Value lies below:\n",
      "        Is value at column: 0<=0.904405?\n",
      "        ___ Value lies below:\n",
      "          Predict [0.]\n",
      "        ^^^ Value lies above:\n",
      "          Predict [1.]\n",
      "      ^^^ Value lies above:\n",
      "        Is value at column: 3<=0.9281949999999999?\n",
      "        ___ Value lies below:\n",
      "          Predict [0.]\n",
      "        ^^^ Value lies above:\n",
      "          Is value at column: 0<=1.5425?\n",
      "          ___ Value lies below:\n",
      "            Predict [1.]\n",
      "          ^^^ Value lies above:\n",
      "            Predict [0.]\n",
      "    ^^^ Value lies above:\n",
      "      Predict [0.]\n"
     ]
    }
   ],
   "source": [
    "tree_printer(my_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is value at column: 0<=0.746345?\n",
      "___ Value lies below:\n",
      "  Is value at column: 1<=5.3646?\n",
      "  ___ Value lies below:\n",
      "    Is value at column: 0<=-0.36205?\n",
      "    ___ Value lies below:\n",
      "      Is value at column: 2<=6.21865?\n",
      "      ___ Value lies below:\n",
      "        Predict [1.]\n",
      "      ^^^ Value lies above:\n",
      "        Is value at column: 1<=-2.2431270000000003?\n",
      "        ___ Value lies below:\n",
      "          Predict [1.]\n",
      "        ^^^ Value lies above:\n",
      "          Predict [0.]\n",
      "    ^^^ Value lies above:\n",
      "      Is value at column: 2<=2.1670999999999996?\n",
      "      ___ Value lies below:\n",
      "        Is value at column: 2<=0.47011500000000006?\n",
      "        ___ Value lies below:\n",
      "          Predict [1.]\n",
      "        ^^^ Value lies above:\n",
      "          Is value at column: 1<=-0.4242105?\n",
      "          ___ Value lies below:\n",
      "            Predict [1.]\n",
      "          ^^^ Value lies above:\n",
      "            Predict [0.]\n",
      "      ^^^ Value lies above:\n",
      "        Predict [0.]\n",
      "  ^^^ Value lies above:\n",
      "    Is value at column: 0<=-3.35385?\n",
      "    ___ Value lies below:\n",
      "      Predict [1.]\n",
      "    ^^^ Value lies above:\n",
      "      Predict [0.]\n",
      "^^^ Value lies above:\n",
      "  Is value at column: 2<=-4.69065?\n",
      "  ___ Value lies below:\n",
      "    Is value at column: 0<=3.91485?\n",
      "    ___ Value lies below:\n",
      "      Predict [1.]\n",
      "    ^^^ Value lies above:\n",
      "      Predict [0.]\n",
      "  ^^^ Value lies above:\n",
      "    Is value at column: 0<=1.7438500000000001?\n",
      "    ___ Value lies below:\n",
      "      Is value at column: 2<=-2.26365?\n",
      "      ___ Value lies below:\n",
      "        Predict [1.]\n",
      "      ^^^ Value lies above:\n",
      "        Is value at column: 3<=0.9281949999999999?\n",
      "        ___ Value lies below:\n",
      "          Predict [0.]\n",
      "        ^^^ Value lies above:\n",
      "          Is value at column: 0<=1.5425?\n",
      "          ___ Value lies below:\n",
      "            Predict [1.]\n",
      "          ^^^ Value lies above:\n",
      "            Predict [0.]\n",
      "    ^^^ Value lies above:\n",
      "      Predict [0.]\n"
     ]
    }
   ],
   "source": [
    "tree_printer(my_tree_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is value at column: 0<=0.295535?\n",
      "___ Value lies below:\n",
      "  Is value at column: 1<=7.4991?\n",
      "  ___ Value lies below:\n",
      "    Is value at column: 0<=-0.4031?\n",
      "    ___ Value lies below:\n",
      "      Is value at column: 2<=6.21865?\n",
      "      ___ Value lies below:\n",
      "        Predict [1.]\n",
      "      ^^^ Value lies above:\n",
      "        Is value at column: 1<=-2.2431270000000003?\n",
      "        ___ Value lies below:\n",
      "          Predict [1.]\n",
      "        ^^^ Value lies above:\n",
      "          Predict [0.]\n",
      "    ^^^ Value lies above:\n",
      "      Is value at column: 1<=5.4422?\n",
      "      ___ Value lies below:\n",
      "        Is value at column: 2<=2.62465?\n",
      "        ___ Value lies below:\n",
      "          Predict [1.]\n",
      "        ^^^ Value lies above:\n",
      "          Is value at column: 0<=-0.365255?\n",
      "          ___ Value lies below:\n",
      "            Predict [1.]\n",
      "          ^^^ Value lies above:\n",
      "            Predict [0.]\n",
      "      ^^^ Value lies above:\n",
      "        Predict [0.]\n",
      "  ^^^ Value lies above:\n",
      "    Is value at column: 0<=-3.9402?\n",
      "    ___ Value lies below:\n",
      "      Predict [1.]\n",
      "    ^^^ Value lies above:\n",
      "      Predict [0.]\n",
      "^^^ Value lies above:\n",
      "  Is value at column: 2<=-4.4434000000000005?\n",
      "  ___ Value lies below:\n",
      "    Is value at column: 0<=3.3408?\n",
      "    ___ Value lies below:\n",
      "      Predict [1.]\n",
      "    ^^^ Value lies above:\n",
      "      Predict [0.]\n",
      "  ^^^ Value lies above:\n",
      "    Is value at column: 0<=1.05795?\n",
      "    ___ Value lies below:\n",
      "      Is value at column: 3<=-0.156695?\n",
      "      ___ Value lies below:\n",
      "        Is value at column: 2<=-3.01805?\n",
      "        ___ Value lies below:\n",
      "          Predict [1.]\n",
      "        ^^^ Value lies above:\n",
      "          Is value at column: 2<=-1.4264999999999999?\n",
      "          ___ Value lies below:\n",
      "            Is value at column: 0<=0.476955?\n",
      "            ___ Value lies below:\n",
      "              Predict [1.]\n",
      "            ^^^ Value lies above:\n",
      "              Predict [0.]\n",
      "          ^^^ Value lies above:\n",
      "            Predict [0.]\n",
      "      ^^^ Value lies above:\n",
      "        Is value at column: 2<=1.9061499999999998?\n",
      "        ___ Value lies below:\n",
      "          Predict [1.]\n",
      "        ^^^ Value lies above:\n",
      "          Predict [0.]\n",
      "    ^^^ Value lies above:\n",
      "      Is value at column: 0<=1.7438500000000001?\n",
      "      ___ Value lies below:\n",
      "        Is value at column: 2<=-2.26365?\n",
      "        ___ Value lies below:\n",
      "          Predict [1.]\n",
      "        ^^^ Value lies above:\n",
      "          Is value at column: 3<=0.90686?\n",
      "          ___ Value lies below:\n",
      "            Predict [0.]\n",
      "          ^^^ Value lies above:\n",
      "            Is value at column: 0<=1.5425?\n",
      "            ___ Value lies below:\n",
      "              Predict [1.]\n",
      "            ^^^ Value lies above:\n",
      "              Predict [0.]\n",
      "      ^^^ Value lies above:\n",
      "        Predict [0.]\n"
     ]
    }
   ],
   "source": [
    "tree_printer(my_tree_gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is value at column: 0<=0.295535?\n",
      "___ Value lies below:\n",
      "  Is value at column: 1<=7.4991?\n",
      "  ___ Value lies below:\n",
      "    Is value at column: 0<=-0.4031?\n",
      "    ___ Value lies below:\n",
      "      Is value at column: 2<=6.21865?\n",
      "      ___ Value lies below:\n",
      "        Predict [1.]\n",
      "      ^^^ Value lies above:\n",
      "        Is value at column: 1<=-2.2431270000000003?\n",
      "        ___ Value lies below:\n",
      "          Predict [1.]\n",
      "        ^^^ Value lies above:\n",
      "          Predict [0.]\n",
      "    ^^^ Value lies above:\n",
      "      Is value at column: 1<=5.4422?\n",
      "      ___ Value lies below:\n",
      "        Is value at column: 2<=2.62465?\n",
      "        ___ Value lies below:\n",
      "          Predict [1.]\n",
      "        ^^^ Value lies above:\n",
      "          Predict [0.]\n",
      "      ^^^ Value lies above:\n",
      "        Predict [0.]\n",
      "  ^^^ Value lies above:\n",
      "    Is value at column: 0<=-3.9402?\n",
      "    ___ Value lies below:\n",
      "      Predict [1.]\n",
      "    ^^^ Value lies above:\n",
      "      Predict [0.]\n",
      "^^^ Value lies above:\n",
      "  Is value at column: 2<=-4.4434000000000005?\n",
      "  ___ Value lies below:\n",
      "    Is value at column: 0<=3.3408?\n",
      "    ___ Value lies below:\n",
      "      Predict [1.]\n",
      "    ^^^ Value lies above:\n",
      "      Predict [0.]\n",
      "  ^^^ Value lies above:\n",
      "    Is value at column: 0<=1.05795?\n",
      "    ___ Value lies below:\n",
      "      Is value at column: 3<=-0.156695?\n",
      "      ___ Value lies below:\n",
      "        Is value at column: 2<=-3.01805?\n",
      "        ___ Value lies below:\n",
      "          Predict [1.]\n",
      "        ^^^ Value lies above:\n",
      "          Predict [0.]\n",
      "      ^^^ Value lies above:\n",
      "        Is value at column: 2<=1.9061499999999998?\n",
      "        ___ Value lies below:\n",
      "          Predict [1.]\n",
      "        ^^^ Value lies above:\n",
      "          Predict [0.]\n",
      "    ^^^ Value lies above:\n",
      "      Is value at column: 0<=1.7438500000000001?\n",
      "      ___ Value lies below:\n",
      "        Is value at column: 2<=-2.26365?\n",
      "        ___ Value lies below:\n",
      "          Predict [1.]\n",
      "        ^^^ Value lies above:\n",
      "          Is value at column: 3<=0.90686?\n",
      "          ___ Value lies below:\n",
      "            Predict [0.]\n",
      "          ^^^ Value lies above:\n",
      "            Is value at column: 0<=1.5425?\n",
      "            ___ Value lies below:\n",
      "              Predict [1.]\n",
      "            ^^^ Value lies above:\n",
      "              Predict [0.]\n",
      "      ^^^ Value lies above:\n",
      "        Predict [0.]\n"
     ]
    }
   ],
   "source": [
    "tree_printer(my_tree_pruned_gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data performance on selected model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model seems to be the 'my_tree_pruned' as it has the highest accuracy. It is also the least complex tree (has the fewest amount of nodes). We now select this decision tree as the best classifier, and we test its performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 54\n",
      "Incorrect predictions: 1\n",
      "The incorrect predictions happened for datapoints: [16]\n",
      "The accuracy of our selected model is: 0.9818181818181818\n"
     ]
    }
   ],
   "source": [
    "accuracy = predict_data_accuracy(test_features, test_labels, my_tree_pruned)\n",
    "print('The accuracy of our selected model is: ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Compare to an existing implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare my model to something more robust. I will use sklearn's .DecisionTreeClassifier to make "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import what we need\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a decision tree with criterion 'entropy' and one with criterion 'gini'\n",
    "tree_sklearn = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "tree_sklearn_gini = DecisionTreeClassifier(criterion = 'gini', random_state = 0)\n",
    "\n",
    "# Fit the two decision trees with the training data\n",
    "tree_sklearn.fit(training_features, training_labels)\n",
    "tree_sklearn_gini.fit(training_features, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the accuracy of the two decision trees\n",
    "accuracy_sklearn = tree_sklearn.score(val_features, val_labels)\n",
    "accuracy_sklearn_gini = tree_sklearn_gini.score(val_features, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the sklearn tree was: 0.9817351598173516\n",
      "The accuracy of the sklearn tree (gini) was: 0.9817351598173516\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracies\n",
    "print('The accuracy of the sklearn tree was: ' + str(accuracy_sklearn))\n",
    "print('The accuracy of the sklearn tree (gini) was: ' + str(accuracy_sklearn_gini))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compare these accuracies to our selected model on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 215\n",
      "Incorrect predictions: 4\n",
      "The incorrect predictions happened for datapoints: [46, 82, 124, 214]\n",
      "The accuracy for my selected model: 0.9817351598173516\n"
     ]
    }
   ],
   "source": [
    "accuracy_pruned = predict_data_accuracy(val_features, val_labels, my_tree_pruned)\n",
    "print('The accuracy for my selected model: ' + str(accuracy_pruned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, they are all equally accurate on the validation data. However, a big problem with my implementation is that it takes around 30 seconds to train and create a decision tree. The Sklearn implementation will fit its classifier within a fraction of a second. The most time consuming part of my code is the determine_optimal_branching() function. This function needs to compute the entropy or the gini index for every single possible split between each unique value for up to a thousand rows over 4 columns (atleast for the root node). The sklearn implementation probably has a much more clever and elegant way of doing this which leads to a more efficient code.\n",
    "\n",
    "Thus, if we had to choose, we should choose the sklearn implementation. In this specific case the accuracies of the tree_sklearn and tree_sklearn_gini are the same. I will just pick the first of the two trees as my final selected model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test accuracy of final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now check the accuracy of my final selected model on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9818181818181818\n"
     ]
    }
   ],
   "source": [
    "accuracy_final_model = tree_sklearn.score(test_features, test_labels)\n",
    "print(accuracy_final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
